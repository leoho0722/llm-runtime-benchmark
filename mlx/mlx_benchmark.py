import argparse
import time
from typing import Any, Dict, List, Tuple

from dotenv import load_dotenv
from mlx import nn
from mlx_lm import load, stream_generate
from mlx_lm.utils import GenerationResponse
from mlx_lm.tokenizer_utils import TokenizerWrapper

load_dotenv(override=True)

# Load Model and Tokenizer
start_model_load_time: float = None
end_model_load_time: float = None
total_model_load_time: float = None

# Model Inference
start_inference_time: float = None
end_inference_time: float = None
total_inference_time: float = None
time_to_first_token: float = None
generation_tokens: int = None
generation_tps: float = None


def load_model(model_id: str) -> Tuple[nn.Module, TokenizerWrapper, float]:
    """Load the model and tokenizer

    Args:
        model_id (`str`): Path to the model

    Returns:
        model (`nn.Module`): MLX Model
        tokenizer (`TokenizerWrapper`): Tokenizer
        total_model_load_time (`float`): Total time taken to load the model
    """

    start_model_load_time = time.perf_counter()

    model, tokenizer = load(model_id)

    end_model_load_time = time.perf_counter()

    total_model_load_time = end_model_load_time - start_model_load_time

    return model, tokenizer, total_model_load_time


def stream_inference(
    model: nn.Module,
    tokenizer: TokenizerWrapper,
    prompt: str,
    max_tokens: int
) -> Dict[str, Any]:
    """Stream Model Inference

    Args:
        model (`nn.Module`): MLX Model
        tokenizer (`TokenizerWrapper`): Tokenizer
        prompt (`str`): Prompt for the LLM model
        max_tokens (`int`): Maximum tokens to generate

    Returns:
        generation_response (`GenerationResponse`): Generation Response
        total_generation_time (`float`): Total time taken for generation
        total_prompt_process_time (`float`): Total time taken to process the prompt
        output (`str`): Output generated by the model
    """

    messages = [{"role": "user", "content": prompt}]
    prompt = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True
    )

    output: str = ""
    for response in stream_generate(model, tokenizer, prompt, max_tokens=max_tokens):
        prompt_tokens = response.prompt_tokens
        prompt_tps = response.prompt_tps
        generation_tokens = response.generation_tokens
        generation_tps = response.generation_tps
        total_prompt_process_time = round(prompt_tokens / prompt_tps, 3)
        total_generation_time = round(generation_tokens / generation_tps, 3)

        # Testing generate Time to First Token for max_tokens = 1
        if max_tokens == 1 and response.finish_reason == "length":
            return {
                "generation_response": response,
                "time_to_first_token": total_generation_time,
                "total_prompt_process_time": total_prompt_process_time,
                "output": output
            }

        if response.finish_reason == "stop":
            return {
                "generation_response": response,
                "total_generation_time": total_generation_time,
                "total_prompt_process_time": total_prompt_process_time,
                "output": output
            }

        if response.text == "<end_of_turn>":
            continue

        output += response.text

        if max_tokens == 1:
            print(response.text, end="")
            print("\n")
        else:
            print(response.text, end="", flush=True)


def parse_args():
    parser = argparse.ArgumentParser(
        description="LLM Benchmark using Apple MLX Framework"
    )
    parser.add_argument(
        "-m", "--model",
        type=str,
        help="Path to the model",
        required=True,
    )
    parser.add_argument(
        "--prompt",
        type=str,
        help="Prompt for the LLM model",
        required=True,
    )
    parser.add_argument(
        "--max_tokens",
        type=int,
        help="Maximum tokens to generate",
        required=True
    )

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    model_id: str = args.model
    prompt: str = args.prompt
    max_tokens: int = args.max_tokens

    print("========== Benchmark Config ==========\n")
    print(f"Model: {model_id}")
    print(f"Prompt: {prompt}")
    print(f"Max Tokens: {max_tokens}")
    print()

    # Load Model and Tokenizer
    model, tokenizer, total_model_load_time = load_model(model_id)

    # Model Inference
    max_new_tokens: List[int] = [1, max_tokens]
    generation_response: GenerationResponse = None
    benchmark_results: Dict[str, Any] = {}
    for max_tokens in max_new_tokens:
        result = stream_inference(
            model,
            tokenizer,
            prompt,
            max_tokens
        )

        generation_response = result["generation_response"]

        if max_tokens == 1 and generation_response.finish_reason == "length":
            time_to_first_token = result["time_to_first_token"]

        benchmark_results = result

    # Show Benchmark Results
    total_prompt_process_time: float = benchmark_results["total_prompt_process_time"]
    prompt_tokens = generation_response.prompt_tokens
    prompt_tps = generation_response.prompt_tps
    total_inference_time: float = benchmark_results["total_generation_time"]
    generation_tokens = generation_response.generation_tokens
    generation_tps = generation_response.generation_tps
    peak_memory = generation_response.peak_memory
    llm_output: str = benchmark_results["output"]

    print("\n\n========== Benchmark Results ==========\n")
    print(f"Total Model Load Time: {total_model_load_time :.2f} s")
    print(f"Prompt Tokens: {prompt_tokens} tokens")
    print(f"Total Prompt Process Time: {total_prompt_process_time :.2f} s")
    print(f"Prompt TPS: {prompt_tps :.2f} tokens/s")
    print(f"Total Inference Time: {total_inference_time :.2f} s")
    print(f"Time to First Token: {time_to_first_token :.2f} s")
    print(f"Generation Tokens: {generation_tokens} tokens")
    print(f"Generation TPS: {generation_tps :.2f} tokens/s")
    print(f"Peak Memory: {peak_memory :.2f} GB")
    print(f"LLM Output: {llm_output}")
